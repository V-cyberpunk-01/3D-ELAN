{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl\n",
    "from dgl.data import MiniGCDataset\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch as th\n",
    "from torch._C import device\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import  Dataset,DataLoader,TensorDataset,random_split\n",
    "from dgl.data import MiniGCDataset\n",
    "from dgl.nn.pytorch import GraphConv\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import sklearn.preprocessing  as skpre\n",
    "import numpy as np\n",
    "from scipy.sparse import coo_matrix, csr_matrix, diags, eye\n",
    "import copy\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error # 均方误差\n",
    "from sklearn.metrics import mean_absolute_error # 平方绝对误差"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_one_graph(adj,features):\n",
    "  fea=[]\n",
    "  for i in range(1,len(features)): \n",
    "    fea.append(features[i].split()[2:])\n",
    "  g = dgl.graph((adj['src'],adj['tgt']))\n",
    "  g.edata['weight']=th.FloatTensor(adj['weight'])\n",
    "  g.edata['r']=th.FloatTensor(np.array(adj[['x','y','z']]))\n",
    "  g.ndata['atom_features']=th.FloatTensor(np.array(fea,dtype='float'))\n",
    "  return g\n",
    "\n",
    "def get_weight_matrix(g):\n",
    "  m=np.zeros((g.num_src_nodes(),g.num_dst_nodes()))\n",
    "  for i in range(m.shape[0]):\n",
    "    for j in range(m.shape[1]):\n",
    "      try:\n",
    "       m[i,j]=g.edges[i,j].data['weight']\n",
    "      except Exception as e:\n",
    "        # print(e)\n",
    "        pass\n",
    "  return m\n",
    "\n",
    "def draw_graph(g):\n",
    "  plt.figure(figsize=(14, 6))\n",
    "  nxg=g.to_networkx(node_attrs=None,edge_attrs=['weight'])\n",
    "  pos=nx.spring_layout(nxg)\n",
    "  colormap= ['r','orange','y','g','b','m','c','darkgrey','grey','dimgrey','black']\n",
    "  # colormap=list(seaborn.xkcd_rgb.keys())[::30]\n",
    "  # colormap.reverse()\n",
    "  weight=nx.get_edge_attributes(nxg,'weight')\n",
    "  edgewidth=list(map(lambda x:1/x,np.array(list(weight.values()))))\n",
    "  # print(np.array(list(nx.get_edge_attributes(nxg,'weight').values())))\n",
    "  # for (u,v,d) in nxg.edges(data=True):\n",
    "  #   print(nxg.get_edge_data(u,v).values(),d['weight'])\n",
    "  le=LabelEncoder()\n",
    "  colors=le.fit_transform(edgewidth)\n",
    "  color=list(map(lambda x:colormap[x],colors))\n",
    "  nx.draw_networkx_edges(nxg,pos,width=edgewidth,edge_color=color) #奇怪的是：edgewidth应该是一个列表，为什么可以直接拿来用呢？Python怎么判断哪个边用的是哪个权重值？\n",
    "  nx.draw_networkx_nodes(nxg,pos)\n",
    "  nx.draw_networkx_labels(nxg,pos)\n",
    "  plt.show()\n",
    "  \n",
    "class normalizer():\n",
    "  def __init__(self,x):\n",
    "    self.matrix=np.array(x,dtype=float)\n",
    "    # print(self.matrix.shape)\n",
    "\n",
    "  def minmax_normalize(self,x=None):  #定义函数，对矩阵数据进行归一化\n",
    "    scaler=skpre.MinMaxScaler()\n",
    "    if x:\n",
    "      return scaler.fit_transform(x,x)\n",
    "    else:\n",
    "      return scaler.fit_transform(self.matrix,self.matrix)\n",
    "      \n",
    "  def absmax_normalize(self,x=None):\n",
    "    scaler=skpre.MaxAbsScaler()\n",
    "    if x:\n",
    "      return scaler.fit_transform(x,x)\n",
    "    else:\n",
    "      return scaler.fit_transform(self.matrix,self.matrix)\n",
    "\n",
    "  def minmax_minus1to1_normalize(self,x=None):\n",
    "    if x:\n",
    "      mean=np.array([0.5]*x.shape[1],dtype=float).reshape(1,-1)\n",
    "      return np.true_divide(self.minmax_normalize(x)-mean,mean)\n",
    "    else:\n",
    "      mean=np.array([0.5]*self.matrix.shape[1],dtype=float).reshape(1,-1)\n",
    "      return np.true_divide(self.minmax_normalize()-mean,mean)\n",
    "      \n",
    "  def standard_normalize(self,x=None):\n",
    "    scaler=skpre.StandardScaler()\n",
    "    if x:\n",
    "      return scaler.fit_transform(x,x)\n",
    "    else:\n",
    "      return scaler.fit_transform(self.matrix,self.matrix)\n",
    "  def reverse_minmax_minus1to1_normalize(self,src_x=None,x:np.array=None):\n",
    "    assert src_x is not None,'source matrix should not be None'\n",
    "    min=np.min(src_x,0)\n",
    "    max=np.max(src_x,0)\n",
    "    if x is not None:\n",
    "      return x*(max-min)+min\n",
    "    else:\n",
    "      return self.matrix*(max-min)+min\n",
    "  @staticmethod\n",
    "  def lambda_max(arr, axis=None, key=None, keepdims=False):\n",
    "    if callable(key):\n",
    "        idxs = np.argmax(key(arr), axis)\n",
    "        if axis is not None:\n",
    "            idxs = np.expand_dims(idxs, axis)\n",
    "            result = np.take_along_axis(arr, idxs, axis)\n",
    "            if not keepdims:\n",
    "                result = np.squeeze(result, axis=axis)\n",
    "            return result\n",
    "        else:\n",
    "            return arr.flatten()[idxs]\n",
    "    else:\n",
    "        return np.amax(arr, axis)\n",
    "  def inverse_absmax_normalize(self,src_x=None,x:np.array=None):\n",
    "    # print(x*self.lambda_max(src_x,0,key=np.abs))\n",
    "    if x is not None:\n",
    "      return x*np.max(np.abs(src_x),0)\n",
    "    else:\n",
    "      return self.matrix*(np.max(np.abs(src_x),0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_file_path='./NbSi3k'\n",
    "feature_file_path=''\n",
    "graphs=[]\n",
    "labels=[]\n",
    "all_adj_m=[]\n",
    "all_feature_m=[]\n",
    "weight_m=None \n",
    "files=os.listdir(feature_file_path)\n",
    "for i,f in enumerate(files):\n",
    "  if i%(int(len(files)/5))==0:\n",
    "    print('{}/{}:{}'.format(i,len(os.listdir(feature_file_path)),f))\n",
    "  with open(os.path.join(feature_file_path,f),'r')as res:\n",
    "    features=res.read().split('\\n')\n",
    "    adj=pd.read_csv(os.path.join(adj_file_path,'adj_'+f.split('.')[0].split('_')[-1]+'.csv'))\n",
    "\n",
    "    g=get_one_graph(adj,features)\n",
    "\n",
    "    graphs.append(g)\n",
    "\n",
    "    if weight_m is None:\n",
    "      weight_m=get_weight_matrix(g)\n",
    "    all_adj_m.append(th.FloatTensor(weight_m))\n",
    "   \n",
    "    na=normalizer(g.ndata['atom_features'].numpy())\n",
    "    all_feature_m.append(th.FloatTensor(na.minmax_normalize())) #特征矩阵归一化处理\n",
    "    labels.append(float(features[0]))\n",
    "draw_graph(g)\n",
    "labels=normalizer(np.array(labels).reshape(-1,1)).absmax_normalize()*10\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_bond_cosines(edges):\n",
    "    '''\n",
    "    计算键角余弦值。\n",
    "    \"\"\"Compute bond angle cosines from bond displacement vectors.\"\"\"\n",
    "    '''\n",
    "    # line graph edge: (a, b), (b, c)\n",
    "    # `a -> b -> c`\n",
    "    # use law of cosines to compute angles cosines\n",
    "    # negate src bond so displacements are like `a <- b -> c`\n",
    "    # cos(theta) = ba \\dot bc / (||ba|| ||bc||)\n",
    "    r1 = -edges.src[\"r\"]\n",
    "    r2 = edges.dst[\"r\"]\n",
    "    bond_cosine = th.sum(r1 * r2, dim=1) / (th.norm(r1, dim=1) * th.norm(r2, dim=1))\n",
    "    bond_cosine = th.clamp(bond_cosine, -1, 1) #限制范围到-1与1之间\n",
    "    return {\"h\": bond_cosine}  #键角信息存在lg的边特征h里面\n",
    "  \n",
    "def get_line_graph(graphs):\n",
    "    '''\n",
    "    获得原子图的线图。\n",
    "    Args:\n",
    "      graph(s):一张图或多个图构成的列表。\n",
    "    '''\n",
    "    if not isinstance(graphs,list):\n",
    "      graphs=list(graphs)\n",
    "    lgs=[]\n",
    "    for g in graphs:\n",
    "        lg = g.line_graph(shared=True,backtracking=False)\n",
    "        lg.apply_edges(compute_bond_cosines)\n",
    "        lgs.append(lg)\n",
    "    return lgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class My_dataset(Dataset):\n",
    "    '''\n",
    "    自定义dataset.\n",
    "    '''\n",
    "    def __init__(self,graphs,labels):\n",
    "        super().__init__()\n",
    "        self.src=graphs\n",
    "        self.trg=labels\n",
    "    def __getitem__(self, index):\n",
    "        return self.src[index], self.trg[index]\n",
    "    def __len__(self):\n",
    "        return len(self.src) \n",
    "      \n",
    "def collate1(samples):\n",
    "    '''\n",
    "    自定义collate函数,针对于输入的数据是矩阵类型的，数据的构成是[((fea,adj), label1), ((fea2,adj2), label2), ...]。\n",
    "    '''\n",
    "    # 输入参数samples是一个列表\n",
    "    # 列表里的每个元素是图和标签对，如[((fea,adj), label1), ((fea2,adj2), label2), ...]\n",
    "    # zip(*samples)是解压操作，解压为graphs:[(fea,adj),(fea2,adj2),...]  labels:[label1, label2, ...]\n",
    "    graphs, labels = map(list, zip(*samples))\n",
    "    # print(graphs)\n",
    "    fea,adj=map(tuple,zip(*graphs)) #再做一次解压，分别得到fea:(fea,fea2,...) adj:(adj,adj2,...)\n",
    "    # print(fea,adj)\n",
    "    return (th.stack(fea,0),th.stack(adj,0)),th.FloatTensor(labels) #在第一个维度上拼接张量\n",
    "  \n",
    "def collate2(samples):\n",
    "    '''\n",
    "    参考collate1函数，区别在于本函数接受的是dgl.graph数据\n",
    "    '''\n",
    "    graphs, labels = map(list, zip(*samples))\n",
    "    gs,lgs=list(map(list,zip(*graphs)))\n",
    "    # dgl.batch 将一批图看作是具有许多互不连接的组件构成的大型图\n",
    "    return (dgl.batch(gs),dgl.batch(lgs)),th.FloatTensor(labels)\n",
    "\n",
    "def get_train_val_test_data(datasets,labels,co_fn,splitmode=1,seed=33,split=[0.7,0.2,0.1],batch_size=1):\n",
    "  '''\n",
    "  生成Dataloader函数。\n",
    "  Args\n",
    "        datasets : 总数据集 [(fea,adj),...]\n",
    "        labels : 标签序列\n",
    "        co_fn : 自定义一个merges a list of samples to form a mini-batch of Tensor(s)的函数\n",
    "        splitmode: 可选参数1或2,表示两种数据loader的生成方式\n",
    "        seed : 随机种子\n",
    "        split : 分割比例。有三种结构可选，总和为1 或者 总和为len(datasets)\n",
    "        batch_size : 指定多少个sample组成一个batch\n",
    "  '''\n",
    "  assert datasets is not None and len(datasets)>0,'数据集不能为空'\n",
    "  lens=len(datasets)\n",
    "  assert abs(sum(split)-1)<1e-5 or sum(split)==lens,'split分割总和不为1或len(datasets)'\n",
    "  if sum(split)!=lens:\n",
    "      split=list(map(lambda x:int(x*lens),split))\n",
    "  split[-1]=lens-sum(split[:-1]) #确保总和为所有数据量 \n",
    "  assert isinstance(batch_size,list) and 1<=min(batch_size) and max(batch_size)<=min(split),'batch_size不是1到min(split)之间的整数'\n",
    "  if splitmode==1:\n",
    "    n_train,n_val,n_test = split\n",
    "    np.random.seed(seed)\n",
    "    idxs = np.random.permutation(lens)  #将原有索引打乱顺序\n",
    "    #计算每个数据集的索引\n",
    "    idx_train = th.LongTensor(idxs[:n_train])\n",
    "    idx_val = th.LongTensor(idxs[n_train:n_train + n_val])\n",
    "    idx_test = th.LongTensor(idxs[n_train + n_val:])\n",
    "    train_data,valid_data,test_data=[My_dataset([datasets[i] for i in j],[labels[i] for i in j]) for j in [idx_train,idx_val,idx_test]]\n",
    "    train_data_loader = DataLoader(train_data, batch_size=batch_size[0], shuffle=True,collate_fn=co_fn,drop_last=True)\n",
    "    valid_data_loader = DataLoader(valid_data, batch_size=batch_size[1], shuffle=True,collate_fn=co_fn,drop_last=True)\n",
    "    test_data_loader = DataLoader(test_data, batch_size=batch_size[2], shuffle=True,collate_fn=co_fn,drop_last=True)\n",
    "    print('测试集下标索引:', idx_test[:10],'...')\n",
    "  else:\n",
    "    mydataset=My_dataset(datasets,labels)\n",
    "    train_data, valid_data,test_data = random_split(mydataset, split)\n",
    "    train_data_loader = DataLoader(train_data, batch_size=batch_size[0], shuffle=True,collate_fn=co_fn,drop_last=True)\n",
    "    valid_data_loader = DataLoader(valid_data, batch_size=batch_size[1], shuffle=True,collate_fn=co_fn,drop_last=True)\n",
    "    test_data_loader = DataLoader(test_data, batch_size=batch_size[2], shuffle=True,collate_fn=co_fn,drop_last=True)\n",
    "  print('mode:{},train:{},valid:{},test:{}'.format(splitmode,len(train_data_loader),len(valid_data_loader),len(test_data_loader)))\n",
    "  return train_data_loader,valid_data_loader,test_data_loader\n",
    "lgs=get_line_graph(graphs) #计算线图\n",
    "a=list(zip(graphs,lgs))\n",
    "# a=list(zip(all_feature_m,all_adj_m))\n",
    "# a=get_line_graph(graphs)\n",
    "\n",
    "split=[0.7,0.2,0.1]\n",
    "bsize=[16,16,1]\n",
    "train_data_loader,valid_data_loader,test_data_loader=get_train_val_test_data(a,labels,splitmode=1,seed=30,batch_size=bsize,co_fn=collate2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl.function as fn\n",
    "from dgl.nn import AvgPooling\n",
    "\n",
    "class RBFExpansion(nn.Module):\n",
    "    \"\"\"Expand interatomic distances with radial basis functions.\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        vmin: float = 0,\n",
    "        vmax: float = 8,\n",
    "        bins: int = 40,\n",
    "        lengthscale = None,\n",
    "    ):\n",
    "        \"\"\"Register th parameters for RBF expansion.\"\"\"\n",
    "        super().__init__()\n",
    "        self.vmin = vmin\n",
    "        self.vmax = vmax\n",
    "        self.bins = bins\n",
    "        self.register_buffer(\n",
    "            \"centers\", th.linspace(self.vmin, self.vmax, self.bins)\n",
    "        )\n",
    "        if lengthscale is None:\n",
    "            # SchNet-style\n",
    "            # set lengthscales relative to granularity of RBF expansion\n",
    "            self.lengthscale = np.diff(self.centers).mean()\n",
    "            self.gamma = 1 / self.lengthscale\n",
    "        else:\n",
    "            self.lengthscale = lengthscale\n",
    "            self.gamma = 1 / (lengthscale ** 2)\n",
    "\n",
    "    def forward(self, distance: th.Tensor) -> th.Tensor:\n",
    "        \"\"\"Apply RBF expansion to interatomic distance tensor.\"\"\"\n",
    "        return th.exp(\n",
    "            -self.gamma * (distance.unsqueeze(1) - self.centers) ** 2\n",
    "        )\n",
    "\n",
    "class EdgeGatedGraphConv(nn.Module):\n",
    "    \"\"\"Edge gated graph convolution from arxiv:1711.07553.\n",
    "    see also arxiv:2003.0098.\n",
    "    This is similar to CGCNN, but edge features only go into\n",
    "    the soft attention / edge gating function, and the primary\n",
    "    node update function is W cat(u, v) + b\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, input_features: int, output_features: int, residual: bool = True\n",
    "    ):\n",
    "        \"\"\"Initialize parameters for ALIGNN update.\"\"\"\n",
    "        super().__init__()\n",
    "        self.residual = residual\n",
    "        # CGCNN-Conv operates on augmented edge features\n",
    "        # z_ij = cat(v_i, v_j, u_ij)\n",
    "        # m_ij = σ(z_ij W_f + b_f) ⊙ g_s(z_ij W_s + b_s)\n",
    "        # coalesce parameters for W_f and W_s\n",
    "        # but -- split them up along feature dimension\n",
    "        self.src_gate = nn.Linear(input_features, output_features)\n",
    "        self.dst_gate = nn.Linear(input_features, output_features)\n",
    "        self.edge_gate = nn.Linear(input_features, output_features)\n",
    "        self.bn_edges = nn.BatchNorm1d(output_features)\n",
    "\n",
    "        self.src_update = nn.Linear(input_features, output_features)\n",
    "        self.dst_update = nn.Linear(input_features, output_features)\n",
    "        self.bn_nodes = nn.BatchNorm1d(output_features)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        g: dgl.DGLGraph,\n",
    "        node_feats: th.Tensor,\n",
    "        edge_feats: th.Tensor,\n",
    "    ) -> th.Tensor:\n",
    "        \"\"\"Edge-gated graph convolution.\n",
    "\n",
    "        h_i^l+1 = ReLU(U h_i + sum_{j->i} eta_{ij} ⊙ V h_j)\n",
    "        \"\"\"\n",
    "        g = g.local_var()\n",
    "\n",
    "        # instead of concatenating (u || v || e) and applying one weight matrix\n",
    "        # split the weight matrix into three, apply, then sum\n",
    "        # see https://docs.dgl.ai/guide/message-efficient.html\n",
    "        # but split them on feature dimensions to update u, v, e separately\n",
    "        # m = BatchNorm(Linear(cat(u, v, e)))\n",
    "        # compute edge updates, equivalent to:\n",
    "        # Softplus(Linear(u || v || e))\n",
    "        g.ndata[\"e_src\"] = self.src_gate(node_feats)\n",
    "        g.ndata[\"e_dst\"] = self.dst_gate(node_feats)\n",
    "        g.apply_edges(fn.u_add_v(\"e_src\", \"e_dst\", \"e_nodes\"))#先将边两边的节点信息聚合到边里面\n",
    "        m = g.edata.pop(\"e_nodes\") + self.edge_gate(edge_feats) #边信息+初始边信息*w得到m矩阵\n",
    "\n",
    "        g.edata[\"sigma\"] = th.sigmoid(m) #m矩阵乘以sigmoid\n",
    "        g.ndata[\"Bh\"] = self.dst_update(node_feats)\n",
    "        g.update_all(\n",
    "            fn.u_mul_e(\"Bh\", \"sigma\", \"m\"), fn.sum(\"m\", \"sum_sigma_h\")\n",
    "        )\n",
    "        g.update_all(fn.copy_e(\"sigma\", \"m\"), fn.sum(\"m\", \"sum_sigma\"))\n",
    "        g.ndata[\"h\"] = g.ndata[\"sum_sigma_h\"] / (g.ndata[\"sum_sigma\"] + 1e-6)\n",
    "        x = self.src_update(node_feats) + g.ndata.pop(\"h\")\n",
    "\n",
    "        # softmax version seems to perform slightly worse\n",
    "        # that the sigmoid-gated version\n",
    "        # compute node updates\n",
    "        # Linear(u) + edge_gates ⊙ Linear(v)\n",
    "        # g.edata[\"gate\"] = edge_softmax(g, y)\n",
    "        # g.ndata[\"h_dst\"] = self.dst_update(node_feats)\n",
    "        # g.update_all(fn.u_mul_e(\"h_dst\", \"gate\", \"m\"), fn.sum(\"m\", \"h\"))\n",
    "        # x = self.src_update(node_feats) + g.ndata.pop(\"h\")\n",
    "\n",
    "        # node and edge updates\n",
    "        x = F.silu(self.bn_nodes(x))\n",
    "        y = F.silu(self.bn_edges(m))\n",
    "\n",
    "        if self.residual:\n",
    "            x = node_feats + x\n",
    "            y = edge_feats + y\n",
    "\n",
    "        return x, y\n",
    "\n",
    "\n",
    "class ALIGNNConv(nn.Module):\n",
    "    \"\"\"Line graph update.\"\"\"\n",
    "    def __init__(self,in_features: int,out_features: int,):\n",
    "        \"\"\"Set up ALIGNN parameters.\"\"\"\n",
    "        super().__init__()\n",
    "        self.node_update = EdgeGatedGraphConv(in_features, out_features)\n",
    "        self.edge_update = EdgeGatedGraphConv(out_features, out_features)\n",
    "\n",
    "    def forward(self,g: dgl.DGLGraph,lg: dgl.DGLGraph,x: th.Tensor,y: th.Tensor,z: th.Tensor,):\n",
    "        \"\"\"Node and Edge updates for ALIGNN layer.\n",
    "        x: node input features\n",
    "        y: edge input features\n",
    "        z: edge pair input features\n",
    "        \"\"\"\n",
    "        g = g.local_var()\n",
    "        lg = lg.local_var()\n",
    "        # Edge-gated graph convolution update on crystal graph\n",
    "        x, m = self.node_update(g, x, y)\n",
    "        \n",
    "        # Edge-gated graph convolution update on crystal graph\n",
    "        y, z = self.edge_update(lg, m, z)\n",
    "\n",
    "        return x, y, z\n",
    "\n",
    "class MLPLayer(nn.Module):\n",
    "    \"\"\"Multilayer perceptron layer helper.\"\"\"\n",
    "    def __init__(self, in_features: int, out_features: int):\n",
    "        \"\"\"Linear, Batchnorm, SiLU layer.\"\"\"\n",
    "        super().__init__()\n",
    "        self.layer = nn.Sequential(\n",
    "            nn.Linear(in_features, out_features),\n",
    "            nn.BatchNorm1d(out_features),\n",
    "            nn.SiLU(),)\n",
    "    def forward(self, x):\n",
    "        \"\"\"Linear, Batchnorm, silu layer.\"\"\"\n",
    "        return self.layer(x)\n",
    "\n",
    "class ALIGNN(nn.Module):\n",
    "    \"\"\"Atomistic Line graph network.\n",
    "    Chain alternating gated graph convolution updates on crystal graph\n",
    "    and atomistic line graph.\n",
    "    \"\"\"\n",
    "    def __init__(self,config):\n",
    "        \"\"\"Initialize class with number of input features, conv layers.\"\"\"\n",
    "        super().__init__()\n",
    "        # print(config)\n",
    "        self.classification = config.classification\n",
    "\n",
    "        self.atom_embedding = MLPLayer(\n",
    "            config.atom_input_features, config.hidden_features)\n",
    "        self.edge_embedding = nn.Sequential(RBFExpansion(vmin=-10,vmax=10.0,bins=config.edge_input_features,),\n",
    "            MLPLayer(config.edge_input_features, config.embedding_features),\n",
    "            MLPLayer(config.embedding_features, config.hidden_features),)\n",
    "        \n",
    "        #键角的embedding方法\n",
    "        self.angle_embedding = nn.Sequential(\n",
    "            RBFExpansion(vmin=-1,vmax=1.0,bins=config.triplet_input_features,),\n",
    "            MLPLayer(config.triplet_input_features, config.embedding_features),\n",
    "            MLPLayer(config.embedding_features, config.hidden_features),)\n",
    "\n",
    "        self.alignn_layers = nn.ModuleList(\n",
    "            [\n",
    "              ALIGNNConv(\n",
    "                config.hidden_features,config.hidden_features,)\n",
    "              for idx in range(config.alignn_layers)\n",
    "              ])\n",
    "        self.gcn_layers = nn.ModuleList(\n",
    "            [\n",
    "                EdgeGatedGraphConv(\n",
    "                    config.hidden_features, config.hidden_features\n",
    "                )\n",
    "                for idx in range(config.gcn_layers)\n",
    "            ])\n",
    "        self.readout = AvgPooling()\n",
    "\n",
    "        if self.classification:\n",
    "            self.fc = nn.Linear(config.hidden_features, 2)\n",
    "          \n",
    "            self.softmax = nn.LogSoftmax(dim=1)\n",
    "        else:\n",
    "            self.fc1 = nn.Linear(config.hidden_features, config.output_features,bias=True)\n",
    "#             self.fc2 = nn.Linear(config.output_features, config.output_features,bias=True)\n",
    "        self.link = None\n",
    "        self.link_name = config.link\n",
    "        if config.link == \"identity\":\n",
    "            self.link = lambda x: x\n",
    "        elif config.link == \"log\":\n",
    "            self.link = th.exp\n",
    "            avg_gap = 0.7  # magic number -- average bandgap in dft_3d\n",
    "            self.fc.bias.data = th.tensor(\n",
    "                np.log(avg_gap), dtype=th.float\n",
    "            )\n",
    "        elif config.link == \"logit\":\n",
    "            self.link = th.sigmoid\n",
    "\n",
    "    def forward(self, g):\n",
    "        \"\"\"ALIGNN : start with `atom_features`\n",
    "        x: atom features (g.ndata)\n",
    "        y: bond features (g.edata and lg.ndata)\n",
    "        z: angle features (lg.edata)\n",
    "        \"\"\"\n",
    "        if len(self.alignn_layers) > 0:\n",
    "            g, lg = g\n",
    "            lg = lg.local_var()\n",
    "            # angle features (fixed)\n",
    "            z = self.angle_embedding(lg.edata.pop(\"h\"))  #在这里把键角信息取出来了\n",
    "#         g = g.local_var()\n",
    "        bondlength = th.norm(g.edata.pop(\"weight\").unsqueeze(1), dim=1)\n",
    "        # initial node features: atom feature network...\n",
    "        x = g.ndata.pop(\"atom_features\")\n",
    "        x = self.atom_embedding(x)\n",
    "        y = self.edge_embedding(bondlength)\n",
    "        for alignn_layer in self.alignn_layers:\n",
    "            x, y, z = alignn_layer(g, lg, x, y, z)\n",
    "\n",
    "        # gated GCN updates: update node, edge features\n",
    "        for gcn_layer in self.gcn_layers:\n",
    "            x, y = gcn_layer(g, x, y)\n",
    "\n",
    "        h = self.readout(g, x)\n",
    "        out = self.fc1(h)\n",
    "        if self.link:\n",
    "            out = self.link(out)\n",
    "        if self.classification:\n",
    "            out = self.softmax(out)\n",
    "        return th.squeeze(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#基本参数设置\n",
    "class config():\n",
    "  def __init__(self):\n",
    "    self.classification=False\n",
    "    self.atom_input_features=48\n",
    "    self.hidden_features=48\n",
    "    self.edge_input_features=20\n",
    "    self.embedding_features=128\n",
    "    self.gcn_layers=2\n",
    "    self.link='identity'\n",
    "    self.triplet_input_features=20\n",
    "    self.alignn_layers=2\n",
    "    self.output_features=1\n",
    "myconfig=config()\n",
    "model=ALIGNN(myconfig)\n",
    "# 定义分类交叉熵损失\n",
    "loss_func = nn.MSELoss().to(device) #nn.CrossEntropyLoss() #\n",
    "# 定义Adam优化器\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "epoch_num=2\n",
    "scheduler=th.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epoch_num, eta_min=0, last_epoch=-1, verbose=False)\n",
    "\n",
    "def evaluate(g,label):\n",
    "  with th.no_grad():\n",
    "    model.eval()\n",
    "    output = model(g)\n",
    "    loss = F.mse_loss(output.view(-1,1), label.view(-1,1)).detach().item()\n",
    "    return loss \n",
    "model.train()\n",
    "train_losses = []\n",
    "val_losses=[]\n",
    "for epoch in range(epoch_num):\n",
    "    train_loss = 0\n",
    "    for iter, (batchg, label) in enumerate(train_data_loader):\n",
    "        batchg, label = (batchg[0].to(device),batchg[1].to(device)), label.to(device)\n",
    "        # label=th.FloatTensor(label).squeeze(0).to(device)\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        # prediction = model(batchg[0].to(device),batchg[1].to(device))#.todense()\n",
    "        prediction = model(batchg)#.todense()\n",
    "        loss= loss_func(prediction.view(-1,1),label.view(-1,1))\n",
    "        # optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.detach().item()\n",
    "    # print(prediction,label) \n",
    "    train_loss /= (iter + 1)\n",
    "    val_loss=0\n",
    "    for iter, (batchg, label) in enumerate(valid_data_loader):\n",
    "      batchg, label = (batchg[0].to(device),batchg[1].to(device)), label.to(device)\n",
    "      # label=th.FloatTensor(label).squeeze()\n",
    "      # vl=evaluate(batchg[0].to(device),batchg[1].to(device),label.to(device)) #.to('cpu')\n",
    "      vl=evaluate(batchg,label) #.to('cpu')\n",
    "      val_loss+=vl\n",
    "    val_loss /= (iter+1)\n",
    "    print('Epoch {}, train_loss:{:.4f} val_loss:{:.4f}'.format(epoch, train_loss,val_loss))\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    scheduler.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_losses(losses,title=['','']):\n",
    "  plt.figure(figsize=(6, 7))\n",
    "  plt.xlabel('epoches', fontsize=10)\n",
    "  plt.ylabel('Loss', fontsize=10)\n",
    "  for i in range(len(title)):\n",
    "    plt.subplot(2,1,i+1)\n",
    "    plt.plot(list(range(1,len(losses[i])+1)),losses[i])\n",
    "    plt.title(title[i])\n",
    "  plt.show()\n",
    "\n",
    "draw_losses([np.array(train_losses),np.array(val_losses)],title=['Train Loss','Val Loss'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=model.to('cpu')\n",
    "model.eval()\n",
    "test_preds, test_labels = [], []\n",
    "with th.no_grad():\n",
    "    for iter, (batchg, label) in enumerate(test_data_loader):\n",
    "        # batchg, label = batchg.to(DEVICE), label.to(DEVICE)\n",
    "        batchg, label = (batchg[0].to(device),batchg[1].to(device)), label.to(device)\n",
    "        prediction = model(batchg)#.todense()\n",
    "        # print('id:',iter,'pred:',prediction.view(-1),'label:',label.view(-1))\n",
    "        # print(prediction.view(1,-1).squeeze(1).numpy(),label)\n",
    "        test_preds.extend(list(prediction.view(-1).numpy()))\n",
    "        test_labels.extend(list(label.view(-1).numpy()))\n",
    "r2=round(r2_score(test_labels,test_preds),4)\n",
    "mae=round(mean_absolute_error(test_labels,test_preds),4)\n",
    "mse=round(mean_squared_error(test_labels,test_preds),4)\n",
    "print('r2_score:',r2,'mse_score:',mse,'mae_score:',mae)\n",
    "# print(test_labels,test_preds)\n",
    "def draw_regression_curve(test_labels,test_preds,score,detail=True):\n",
    "  plt.figure(figsize=(8, 8))\n",
    "  plt.subplot(2,1,1)\n",
    "  plt.subplots_adjust(wspace=0.2,hspace=0.3)\n",
    "  plt.xlabel('Data ID', fontsize=10)\n",
    "  plt.ylabel('Value', fontsize=10)\n",
    "  plt.plot(range(len(test_preds)),test_preds, marker='o',color='b',label='prediction')\n",
    "  plt.plot(range(len(test_preds)),test_labels,marker='x',color='r',label='label')\n",
    "  plt.title('Regression,R2_score:{:.3f},MSE:{:.3f},MAE:{:.3f}'.format(*score))\n",
    "  plt.legend()\n",
    "  plt.grid()\n",
    "  if detail:\n",
    "    plt.subplot(2,1,2)\n",
    "    plt.xlabel('Prediction Value', fontsize=10)\n",
    "    plt.ylabel('Label Value', fontsize=10)\n",
    "    plt.scatter(test_preds,test_labels)\n",
    "    plt.plot(range(-10,11),range(-10,11))\n",
    "    plt.title('Prediction-Label Scatter')\n",
    "    plt.grid()\n",
    "  plt.show()\n",
    "draw_regression_curve(test_labels,test_preds,[r2,mse,mae])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
